\documentclass[letterpaper]{article}
\usepackage{fullpage}
\addtolength{\hoffset}{-.5in}
\addtolength{\textwidth}{1in}
\addtolength{\voffset}{-.5in}
\addtolength{\textheight}{1in}
\begin{document}


<<startup, echo=FALSE, message=FALSE>>=
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=100), options(width=80))
# DCL e152 piloting. started 4 October 2022 by Jo Etzel
# fixed movie stat input files, 10 October 2022

library(RNifti)
library(stringr)
rm(list = ls())
# safety option
options(warnPartialMatchDollar = TRUE)

# root directory
in.path <- "/data/nil-external/dcl/Events152_fMRI_NeuralMechanisms/voxelwiseAnalyses/"

sub.ids <- c(paste0("0", 1:9), 10:47);
run.ids <- 1:4;
nframes <- c(405, 467, 404, 444);  # number of frames in each run, run.ids order
movie.ids <- c("1.2.3_C1_trim", "6.3.9_C1_trim", "3.1.3_C1_trim", "2.4.1_C1_trim");  # clip names in run.ids order; see analysisPrep.R
mov.ids <- c("1.2.3", "6.3.9", "3.1.3", "2.4.1");  # clip names in run.ids order; see analysisPrep.R

## Feature-specific parameters can be changed here to switch between PE and visual features (e.g. pixel change mean)
# features <- c("pe")
features <- c("pixel_change_mean")
# first and last TR of movie stat timeseries to correlate with parcel timeseries
first <- 1
last <- 500  # this number can exceed nframes, which is ok.
# input directory of the convoluted feature timeseries
# in_feature_dir <- paste0(in.path, "matlab_resampling2/out_convo_pe/")
in_feature_dir <- paste0(in.path, "matlab_resampling2/out_convo_visual/")
# directory for output correlation between feature timeseries and parcel timeseries
# out_corr_dir <- paste0(in.path, "knitr_tan/median_correlation_with_shifts/pe_corrs_", first, "_", last, "/")
out_corr_dir <- paste0(in.path, "knitr_tan/median_correlation_with_shifts/pcm_corrs_", first, "_", last, "/")
# check if out_corr_dir exists
if (!file.exists(out_corr_dir)) {
  dir.create(out_corr_dir)
}

onset.tbl <- read.table(paste0(in.path, "e152onsets.txt"));  # delay between first TR and movie onset, in sec
#  head(onset.tbl)
#   sub.id sub.lbl run1_1.2.3 run2_6.3.9 run3_3.1.3 run4_2.4.1
# 1 sub-01 e152003   7.900409   8.328984   7.929364   8.195244
# 2 sub-02 e152004   8.856364   8.909626   8.313563   8.564747
# feature.onsets <- c(6.16, 7.88, 5.52, 6.82) # in seconds, the delay between movie onset and PE onset.
feature.onsets <- c(0, 0, 0, 0) # in seconds, if we use visual features such as pixel change, onset.tbl is enough, so 



suff_sch <- "np2_Sch400x7";  # last bit of the input filenames
suff_subcortical <- "np2_subcortical";  # last bit of the input filenames
TR <- 1.483;   # TR in seconds
#do.trim <- 0.1;
get.FTrz <- function(in.val) {
  return(.5 * log((1 + in.val) / (1 - in.val)))
} # Fisher's r-to-z transformation.
get.FTzr <- function(in.val) {
  return((exp(2 * in.val) - 1) / (exp(2 * in.val) + 1))
} # Fisher's z-to-r transformation

source("/data/nil-external/ccp/JosetAEtzel/DMCC_files/niftiPlottingFunctions.R");

under.img <- readNifti(paste0(in.path, "HCP_S1200T1w_94x111x94.nii.gz")); # made in analysisPrep.R
sch.img <- readNifti(paste0(in.path, "Schaefer2018_400x7_94x111x94.nii.gz"));
sub.img <- readNifti(paste0(in.path, "subcortical_94x111x94.nii.gz"));
subcort.tbl <- read.csv(paste0(in.path, "subcorticalKey.csv"), stringsAsFactors=FALSE);  # subcortical_94x111x94.nii.gz parcel labels

sch.ids <- 1:400;   # Schaefer2018_400x7 parcellation

# # Schaefer parcellation labels for each parcel, useful for plotting
# # this file is fetched from github.
# fname <- paste0(in.path, "knitr_tan/median_correlation_with_shifts/Schaefer2018_400Parcels_7Networks_order_info.txt")
# if (file.exists(fname)) {
#   fin <- file(fname, "rt")
#   tmp <- readLines(fin)
#   close(fin)
#   unlink(fin)
#   if (length(tmp) != 800) {
#     stop("not expected Schaefer key.")
#   }
#   tmp <- tmp[seq(from = 1, to = 800, by = 2)] # every-other entry is a label
#   p.key <- gsub("7Networks_", "", tmp)
# } else {
#   print(paste("Not exist", fname))
# }
# # extract out the network part of each parcel name
# tmp <- strsplit(p.key, "_");
# network.key <- rep(NA, length(p.key));
# for (i in 1:length(p.key)) {
#   network.key[i] <- tmp[[i]][2]
# }
# # length(unique(network.key))    # [1] 7
# network.ids <- unique(network.key)
# # length(network.ids)  # [1] 400
# # export to a txt file, easier for other plotting code to use
# write.table(network.key, paste0(in.path, "knitr_tan/median_correlation_with_shifts/network_key_Schaefer_400Parcels.txt"), row.names = FALSE, col.names = FALSE)

plot.slices <- round(seq(from=27, to=72, length.out=10));   # slices to show


# column names and order of the movie-stat input files
#mstat.ids <- c("time", "pixel_change_mean", "pixel_change_var" , "luminance_mean", "luminance_var");

# for median/mean metrics, the range can be small, like -.05 to .2, but for individual subjects, we'd want a bigger range, like -.1 to .5
nlims <- c(-0.1, -0.5)
plims <- c(0.1, 0.5)   # used in the brain-plotting code, too.

# This function load the parcel-avg and pe correlation (created by corr.parcel.pe) and plot correlation for each subject.
just.plot.subject <- function(do.col, do.start, rid, sid) {
  # do.col <- "pe"; do.start <- 0; rid <- 1; sid <- 1
  # print(paste(do.col, " run", movie.ids[rid], "at various starts (starts in TR)"));
  temp.img <- array(0, dim(under.img))
  for (do.par in c("Schaefer2018_400x7", "subcortical")) {
    # specify parcel ids, parcel-pe correlations, and "under" img to plot (Schaefer or subcortical)
    if (do.par == "Schaefer2018_400x7") {
      p.lbls <- sch.ids # 1:400
      p.img <- sch.img
      fname <- paste0(out_corr_dir, "matlabCorrs_", do.col, "_run", rid, "_dtFixed.txt")
      # print(paste("Loading", fname));
      cor.tbl <- read.table(fname)
    }
    if (do.par == "subcortical") {
      p.lbls <- subcort.tbl$HCP.label
      p.img <- sub.img
      fname <- paste0(out_corr_dir, "matlabCorrs_", do.col, "_run", rid, "_dtFixed_subcortical.txt")
      cor.tbl <- read.table(fname)
    }
    # use column name since parcel order not fixed for subcortical
    # for (pid in 1:length(p.lbls)) {
    for (pid in p.lbls) {
      # vals <- cor.tbl[which(cor.tbl$start.at == paste0("corrAt", do.start)),paste0("p", pid)];
      # temp.img[which(p.img == pid)] <- median(vals);
      # sid_str <- "01";
      sid_str <- str_pad(sid, width = 2, pad = "0")
      val <- cor.tbl[which((cor.tbl$start.at == paste0("corrAt", do.start)) & (cor.tbl$sub.id == paste0("sub-", sid_str))), paste0("p", pid)]
      temp.img[which(p.img == pid)] <- val
    }
  }
  plot.volume(temp.img, under.img, neg.lims = nlims, pos.lims = plims, ttl = paste0(do.start, movie.ids[rid], do.col))
}

# This function load the parcel-avg and pe correlation (created by corr.parcel.pe) and plot median correlation across subjects.
just.plot.median <- function(do.col, do.start, rid) {   # do.col <- "pe"; do.start <- 0; rid <- 1;
    # print(paste(do.col, " run", movie.ids[rid], "at various starts (starts in TR)"));
    temp.img <- array(0, dim(under.img));
    for (do.par in c("Schaefer2018_400x7", "subcortical")) {
      if (do.par == "Schaefer2018_400x7") {
        p.lbls <- sch.ids; # 1:400
        p.img <- sch.img;
  # do.start <- 0; rid <- 1; do.col <- "pixel_change_mean";
  # made above; do.start must be in the .txt
        fname <- paste0(in.path, "knitr_tan/median_correlation_with_shifts/PECorrs/matlabCorrs_", do.col, "_run", rid, "_dtFixed.txt");
        # print(paste("Loading", fname));
        cor.tbl <- read.table(fname);
      }
      if (do.par == "subcortical") {
        p.lbls <- subcort.tbl$HCP.label
        p.img <- sub.img;
        # do.start <- 0; rid <- 1; do.col <- "pixel_change_mean";
        # made above; do.start must be in the .txt
        fname <- paste0(in.path, "knitr_tan/median_correlation_with_shifts/PECorrs/matlabCorrs_", do.col, "_run", rid, "_dtFixed_subcortical.txt")
        cor.tbl <- read.table(fname)
      }
              # use column name since parcel order not fixed for subcortical
      # for (pid in 1:length(p.lbls)) {
      for (pid in p.lbls) {
        vals <- cor.tbl[which(cor.tbl$start.at == paste0("corrAt", do.start)), paste0("p", pid)];
        temp.img[which(p.img == pid)] <- median(vals);
      }
    }
  plot.volume(temp.img, under.img, neg.lims=nlims, pos.lims=plims, ttl=paste0(do.start, movie.ids[rid], " ", do.col));
}


# This function load parcel-average timeseries for each subject, shift them and correlate it with PE timeseries.
# All correlations are saved in a single .txt file
corr.parcel.pe <- function(do.col, do.starts, rid, from, to, sub.ids) {
  # do.starts <- 0:1; rid <- 1; do.col <- "pe";   # which column of the movie-stat file to correlate the BOLD against, rid is movie id.
  # from <- 1; to <- 100;   # the duration of the movie in TRs to correlate the BOLD against.
  for (do.par in c("Schaefer2018_400x7", "subcortical")) {
    if (do.par == "Schaefer2018_400x7") {
      fname.out <- paste0(out_corr_dir, "matlabCorrs_", do.col, "_run", rid, "_dtFixed.txt")
      p.lbls <- sch.ids
      suff <- suff_sch
    }
    if (do.par == "subcortical") {
      fname.out <- paste0(out_corr_dir, "matlabCorrs_", do.col, "_run", rid, "_dtFixed_subcortical.txt")
      p.lbls <- subcort.tbl$HCP.label
      suff <- suff_subcortical
    }
    # fname.out <- paste0(in.path, "knitr_tan/median_correlation_with_shifts/PECorrs/matlabCorrs_", do.col, "_run", rid, "_dtFixed.txt");
    if (!file.exists(fname.out)) { # correlate parcel timeseries of all subjects with PE and save
      # load in the parcel-average timeseries of all subjects for each run & store so don't need to read over and over
      nps.lst <- vector("list", length(sub.ids))
      for (sid in 1:length(sub.ids)) { # sid <- 1;
        # path to parcel-average timeseries of this subject, before xcp processing
        fname <- paste0(in.path, "np2/sub-", sub.ids[sid], "_run-", rid, "_", suff, ".txt")

        if (file.exists(fname)) {
          np.tbl <- read.delim(fname)
          np.tbl <- np.tbl[, -c(1, 2)] # take off first two label columns
          if (nrow(np.tbl) != nframes[rid] | ncol(np.tbl) != length(p.lbls)) {
            print(paste(nrow(np.tbl), nframes[rid], ncol(np.tbl), length(p.lbls)))
            stop("wrong np.tbl.")
          }
          sub.onset <- onset.tbl[which(onset.tbl$sub.id == paste0("sub-", sub.ids[sid])), paste0("run", rid, "_", mov.ids[rid])] # onset in seconds
          feature.onset <- feature.onsets[rid]
          total_onset <- sub.onset + feature.onset
          TR.onset <- round(total_onset / TR) # onset in TR, combining subject (scanning started before movie onset) and pe onset (pe starts not at movie onset but later).

          nps.lst[[sid]] <- np.tbl[(TR.onset:nrow(np.tbl)), ] # keep frames, starting at movie + PE start
        } else {
          stop("missing???")
        }
      }
      rm(np.tbl, fname) # cleanup

      # load convoluted PE for this movie, used for all people
      feature.vec <- read.csv(paste0(in_feature_dir, "conv_", mov.ids[rid], "_", do.col, "_dtFix.csv"), header = FALSE)
      feature.vec <- unlist(feature.vec[1, ], use.names = FALSE)

      # one output file, set of starts
      # trim 5-10 points off the front of the movie timeseries to avoid the starting big-dip????
      out.tbl <- data.frame(array(NA, c(length(sub.ids) * length(do.starts), length(p.lbls) + 2)))
      # column names are parcel numbers
      colnames(out.tbl) <- c("sub.id", "start.at", paste0("p", p.lbls))
      ctr <- 1 # counter = #subs * #starts
      for (sid in 1:length(sub.ids)) { # sid <- 2;
        # already trimmed pre-onset frames from the parcel-average timeseries
        # sub.onset <- onset.tbl[which(onset.tbl$sub.id == paste0("sub-", sub.ids[sid])), paste0("run", rid, "_", movie.ids[rid])];

        for (do.start in do.starts) { #  sid <- 2;  do.start <- 0;
          feature.vec2 <- feature.vec # reset
          # feature.vec is a vector, so nrow(feature.vec) return NULL, use length instead.
          feature.vec2 <- feature.vec2[from: min(to, length(feature.vec2))] # trim off the movie

          # each element of nps.lst is a table (np.tbl), nrow is #frames, ncol is #parcels
          do.TRs <- (1 + do.start + from - 1):nrow(nps.lst[[sid]]) # frames in this run, starting at do.start

          if (length(do.TRs) < length(feature.vec2)) {
            feature.vec2 <- feature.vec2[1:length(do.TRs)]
          } # trim end of movie so vectors match length
          if (length(feature.vec2) < length(do.TRs)) {
            do.TRs <- do.TRs[1:length(feature.vec2)]
          }

          out.tbl$sub.id[ctr] <- paste0("sub-", sub.ids[sid])
          out.tbl$start.at[ctr] <- paste0("corrAt", do.start)
          for (pid in 1:length(p.lbls)) {
            corr <- cor(nps.lst[[sid]][do.TRs, pid], feature.vec2, method = "pearson")
            # check if corr is NA
            if (is.na(corr)) {
              print("corr is NA, check values below for clues")
              print(paste0("sid=", sid, " do.start=", do.start, " do.TRs=", length(do.TRs), " feature.vec2=", length(feature.vec2)))
            }
            out.tbl[ctr, pid + 2] <- corr
          }
          # print(paste0("corrs=", out.tbl[ctr, 1:ncol(out.tbl)]))
          ctr <- ctr + 1
        }
      }
      print(paste("Saving", fname.out))
      write.table(out.tbl, fname.out)
    } else {
      print(paste("Existed, skip:", fname.out))
    }
  }
  # for (do.start in do.starts) {
  #   just.plot(do.col, do.start, rid)
  # }
}


@

\noindent compiled \today\  \par
\noindent DCL events152 positive controls, with matlab convolution and downsampling. \par
\noindent files under \texttt{/data/nil-external/dcl/Events152\textunderscore fMRI\textunderscore NeuralMechanisms/voxelwiseAnalyses/} \par
\vspace{0.2 cm} 
\noindent 47 people watched movies in four fMRI runs; everyone watched the same movie in each run (four different movies total; same order for everyone). Images were preprocessed with fmriprep (volumes only), then np2 detrended and normalized, and finally parcel-averaged using the Schaefer2018 400x7 parcellation (\texttt{analysisPrep.R}). The runs had the same number of volumes for everyone but the movie onsets varied a bit (7.9 to 11.4 seconds); see \texttt{e152onsets.txt} and \texttt{getOnsets.R}. \par
\vspace{0.2 cm}
\noindent The correlations here follow the same logic as \texttt{/knitr/parcelCorrelations/movieParcelCorrelations.rnw}, but with Matt's matlab code for convolving and downsampling the framewise movie measures, \texttt{/matlab\textunderscore resampling/Bezdek\textunderscore resampling.m}. Tan Nguyen created the files with PE statistic experienced by SEM model for each frame of each movie. These stats are every 0.333 seconds, much faster than the BOLD or the TR (1.483 sec). \par
\vspace{0.2 cm}
\noindent Aligning the movie and BOLD timeseries properly before correlating has been a challenge, but seems to be sorted now. The first section gives shows the median correlation for each movie statistic and run at offset 0, which should be (and is) best. A selection of the other offsets are shown on later pages, and it's clear the correlation gets worse as the offset increases.   \par




\newpage
\noindent Correlation  \par
\vspace{0.2 cm} 
<<code2legend, echo=FALSE, dev='pdf', fig.height=0.3, fig.width=7.5, fig.align="center">>=
par(oma = rep(0.2, 4), mar = rep(0, 4), mgp = rep(0, 3))

#nlims <- c(-0.2,-0.05); plims <- c(0.05, 0.2);   # used in the brain-plotting code, too.

# blank plot
plot(x = 0, y = 0, xlim = c(-10, 131), type = "n", ylab = "", xlab = "", main = "", bty = "n", xaxt = "n", yaxt = "n")
for (i in 1:length(cols.warm)) {
  rect(xleft = 66 + i, xright = 67 + i, ybottom = -0.5, ytop = 0.5, col = cols.warm[i], border = cols.warm[i])
}
for (i in 1:length(cols.cool)) {
  rect(xleft = 53 - i, xright = 54 - i, ybottom = -0.5, ytop = 0.5, col = cols.cool[i], border = cols.cool[i])
}
text(x=-8, y=0, labels=nlims[1], adj=1, cex=0.7);   # adj=1 for right-justified text; adj=0 for left-justified
text(x=53.5, y=0, labels=nlims[2], adj=0, cex=0.7);
#text(x=60, y=0, labels=0, cex=0.8);
text(x=66, y=0, labels=plims[1], adj=1, cex=0.7);
text(x=128, y=0, labels=plims[2], adj=0, cex=0.7);

@
\vspace{0.2 cm}
<<code2, dev='pdf', cache=TRUE, echo=FALSE, fig.height=1, fig.width=8, fig.align='center'>>=

#for (i in 1:4) { just.plot("pixel_change_mean", 0, i); }
for (do.col in features) {
  for (i in 1:4) {
    print(paste("calculate", do.col, "correlations for run", movie.ids[i], "at various starts (starts in TR)"))
    corr.parcel.pe(do.col, 0:0, i, first, last, sub.ids)
  }
}
@

\newpage
\noindent Correlation  \par
\vspace{0.4 cm}

<<code4legend, echo=FALSE, dev='pdf', fig.height=0.3, fig.width=7.5, fig.align="center">>=
par(oma = rep(0.2, 4), mar = rep(0, 4), mgp = rep(0, 3))

# blank plot
plot(x = 0, y = 0, xlim = c(-10, 131), type = "n", ylab = "", xlab = "", main = "", bty = "n", xaxt = "n", yaxt = "n")
for (i in 1:length(cols.warm)) {
  rect(xleft = 66 + i, xright = 67 + i, ybottom = -0.5, ytop = 0.5, col = cols.warm[i], border = cols.warm[i])
}
for (i in 1:length(cols.cool)) {
  rect(xleft = 53 - i, xright = 54 - i, ybottom = -0.5, ytop = 0.5, col = cols.cool[i], border = cols.cool[i])
}
text(x=-8, y=0, labels=nlims[1], adj=1, cex=0.7)   # adj=1 for right-justified text; adj=0 for left-justified
text(x=53.5, y=0, labels=nlims[2], adj=0, cex=0.7)
#text(x=60, y=0, labels=0, cex=0.8);
text(x=66, y=0, labels=plims[1], adj=1, cex=0.7)
text(x=128, y=0, labels=plims[2], adj=0, cex=0.7)

@

\vspace{0.2 cm}


<<code4, dev='pdf', cache=TRUE, echo=FALSE, fig.height=1, fig.width=8, fig.align='center', results='asis', eval=TRUE>>=
# https://bookdown.org/yihui/rmarkdown-cookbook/results-asis.html: By default, text output from code chunks will be written out verbatim with two leading hashes (see Section 11.12). The text is verbatim because knitr puts it in fenced code blocks (```). If you do not want the text to be in a fenced code block (or the leading hashes), you can set the chunk option results='asis'. This is useful to programmatically break pages using cat("\n\\newpage\n"). However, it will break other printing functions, either cat or print, if the string contain "LaTEX" characters such as \ or _ (e.g. pixel_change_mean). Notice, \n will break print but not cat because print also add quotes ("") and [1] in front of the string (e.g. [1] "haha"), and "\n is an incorrect syntax. If print is used, make sure there is no \n in the string.


cat("\n\nGrouping by Movie")
# grouping by movie
for (do.col in features) {
  str <- paste("\n\ncalculate", do.col, "correlations with parcel timeseries")
  # replace underscores with spaces
  str <- gsub("_", " ", str)
  cat(str)
  for (i in 1:4) {
    for (sid in 1:length(sub.ids)) {
      str <- paste("\n\nsubjects", sid, "run", mov.ids[i])
      str <- gsub("_", " ", str)
      cat(str)
      just.plot.subject(do.col, 0, i, sid)
    }
    cat("\n\\newpage\n")
  }
}

# print("Grouping by Subject")
# # grouping by subject
# for (do.col in features) {
#   print(paste(do.col, "stats"));
#   for (sid in 1:47) {
#     for (i in 1:4) {
#       print(paste("subject", sid, "run", mov.ids[i], do.col, "stats"), quote=FALSE);
#       just.plot.subject(do.col, 0, i, sid);
#     }
#     cat("\n\\newpage\n");
#   }
# }
@


\end{document}